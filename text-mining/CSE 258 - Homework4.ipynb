{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict # Dictionaries with default values\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDataFromFile(fname):\n",
    "  for l in open(fname):\n",
    "    yield ast.literal_eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the first 10,000 reviews from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data...\")\n",
    "data = list(parseDataFromFile(\"train_Category.json\"))[:10000]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_votes': 0,\n",
       " 'review_id': 'r24440074',\n",
       " 'user_id': 'u08070901',\n",
       " 'review_text': 'Pretty decent. The ending seemed a little rush but a good ending to the first trilogy in this series. The fact that most of the time it is a military fantasy makes it interesting. Also all of the descriptions of food just make me hungry.',\n",
       " 'rating': 5,\n",
       " 'genreID': 2,\n",
       " 'genre': 'fantasy_paranormal'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [d['review_text'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pretty decent. The ending seemed a little rush but a good ending to the first trilogy in this series. The fact that most of the time it is a military fantasy makes it interesting. Also all of the descriptions of food just make me hungry.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read the reviews without capitalization or punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73286\n"
     ]
    }
   ],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation) # give the all sets of punctuation.\n",
    "for corp in corpus:\n",
    "    r = ''.join([c for c in corp.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        wordCount[w] += 1\n",
    "\n",
    "print(len(wordCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_uni = [(wordCount[w], w) for w in wordCount]\n",
    "counts_uni.sort()\n",
    "counts_uni.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(73431, 'the'), (44301, 'and'), (39577, 'a'), (36821, 'to'), (36581, 'i')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_uni[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus: each review with capitalization and punctuation\n",
    "# texts: each review without capitalization and punctuation\n",
    "\n",
    "# ---- unigram ----\n",
    "# wordCount: repeat times of unigram(unsorted)\n",
    "# counts_uni: sorted unigram repeat times \n",
    "# df: frequency of every word, regradless of repeat in one review\n",
    "# uni_1k: top 1k unigrams\n",
    "# uni_Id: top 1k bigrams with index\n",
    "\n",
    "# ---- bigram ----\n",
    "# bigrams_count: repeat times of bigram(unsorted)\n",
    "# counts_bi: sorted bigram repeat times \n",
    "# words: top 1k bigrams\n",
    "# wordId: top 1k bigrams with index\n",
    "\n",
    "# ---- uni&bi ----\n",
    "# bigrams_count(update): repeat times of unigram&bigram(unsorted)\n",
    "# countAll: repeat times of uni&bi(sorted)\n",
    "# words_all: top 1k unigram&bigram\n",
    "# wordId_all: top 1k unigram&bigram with index\n",
    "\n",
    "# ---- function ----\n",
    "# puncFilter: eliminate the punctutation and capitalization\n",
    "# bigrams: each one of it is a bigram\n",
    "# feature: bigram-count-feat\n",
    "# feature_combine: unigram&bigram-count-feat\n",
    "# feature_idf: unigram-idf-feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    How many unique bigrams are there amongst the reviews?  List the 5 \n",
    "    most-frequently-occurring bigrams along with their number of occurrences \n",
    "    in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string.punctuation: give the sets of all punctuation.\n",
    "punctuation = set(string.punctuation) \n",
    "\n",
    "def puncFilter(data, remove): # remove the punctuation or not\n",
    "    if remove == True:\n",
    "        return ''.join([c for c in data.lower() if not c in punctuation]).split()\n",
    "    else:\n",
    "        return ' '.join(re.findall(r\"\\w+|[^\\w\\s]\", data.lower())).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(text, remove):\n",
    "    return nltk.bigrams(puncFilter(text, remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genuinely', 'enthralling'),\n",
       " ('enthralling', 'if'),\n",
       " ('if', 'collins'),\n",
       " ('collins', 'or'),\n",
       " ('or', 'bernard')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = bigrams(corpus[0], True)\n",
    "lst = list(test)\n",
    "lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_count = defaultdict(int)\n",
    "for text in corpus:\n",
    "    r = list(bigrams(text, True))\n",
    "    for w in r:\n",
    "        bigrams_count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts_bi = [(bigrams_count[w], w) for w in bigrams_count]\n",
    "counts_bi.sort()\n",
    "counts_bi.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "521502"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counts_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7927, ('of', 'the')),\n",
       " (5850, ('this', 'book')),\n",
       " (5627, ('in', 'the')),\n",
       " (3189, ('and', 'the')),\n",
       " (3183, ('is', 'a'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_bi[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The code provided performs least squares using the 1000 most common unigrams.\n",
    "    Adapt it to use the 1000 most common bigrams and report the MSE obtained using \n",
    "    the new predictor (use bigrams only, i.e., not unigrams+bigrams) (1 mark). Note \n",
    "    that the code performs regularized regression with a regularization parameter of\n",
    "    1.0. The prediction target should be the ‘rating’ field in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 'the'), ('this', 'book'), ('in', 'the'), ('and', 'the'), ('is', 'a')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [x[1] for x in counts_bi[:1000]]\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('genuinely', 'enthralling'),\n",
       " ('enthralling', 'if'),\n",
       " ('if', 'collins'),\n",
       " ('collins', 'or'),\n",
       " ('or', 'bernard')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = list(bigrams(corpus[0], True))\n",
    "r[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordId = dict(zip(words, range(len(words)))) # from 0 to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [0]*len(words) # 长度为1000\n",
    "    r = list(bigrams(datum, True))\n",
    "    for w in r:\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = feature(corpus[0])\n",
    "len(corpus), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(c) for c in corpus]\n",
    "y = [d['rating'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.76533043, 3.45717033, 3.50221229, 3.72832576, 3.56647851]),\n",
       " [5, 5, 4, 5, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMSE(predictions, y):\n",
    "    return numpy.mean((y - predictions)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0178804824879226\n"
     ]
    }
   ],
   "source": [
    "print(getMSE(predictions, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Repeat the above experiment using unigrams and bigrams, still \n",
    "    considering the 1000 most common. That is, your model will \n",
    "    still use 1000 features (plus an offset), but those 1000 \n",
    "    features will be some combination of unigrams and bigrams. \n",
    "    Report the MSE obtained using the new predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_count.update(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73286, 594788)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordCount), len(bigrams_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "countAll = [(bigrams_count[w], w) for w in bigrams_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "countAll.sort(key=lambda t: t[0])\n",
    "countAll.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11131, 'with'),\n",
       " (9638, 'her'),\n",
       " (9138, 'as'),\n",
       " (7927, ('of', 'the')),\n",
       " (7207, 'on')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countAll[15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all = [x[1] for x in countAll[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['with', 'her', 'as', ('of', 'the'), 'on']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_all[15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordId_all = dict(zip(words_all, range(len(words_all))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_combine(datum):\n",
    "    feat = [0]*len(words) # 长度为1000\n",
    "    r = list(bigrams(datum, True))\n",
    "    for w in r:\n",
    "        if w in words_all:\n",
    "            feat[wordId_all[w]] += 1\n",
    "    \n",
    "    r = list(puncFilter(datum, True))\n",
    "    for w in r:\n",
    "        if w in words_all:\n",
    "            feat[wordId_all[w]] += 1\n",
    "    \n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature_combine(c) for c in corpus]\n",
    "y = [d['rating'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 9, 14, 3, 2, 9, 2, 2, 4, 2]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9683729530414934\n"
     ]
    }
   ],
   "source": [
    "print(getMSE(predictions, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    What is the inverse document frequency of the words ‘stories’, ‘magician’, \n",
    "    ‘psychic’, ‘writing’, and ‘wonder’? What are their tf-idf scores in the \n",
    "    first review (using log base 10, following the first definition of tf-idf \n",
    "    given in the slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of every word, regradless of repeat in one review\n",
    "\n",
    "df = defaultdict(int)\n",
    "for c in corpus:\n",
    "    r = list(puncFilter(c, True))  # based on unigram\n",
    "    words = set(r)\n",
    "    for w in words:\n",
    "        df[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse document frequency\n",
    "\n",
    "def findIdf(word):\n",
    "    f = df[word]\n",
    "    if f == 0:\n",
    "        return log(len(corpus), 10)\n",
    "    return log(len(corpus) / float(f), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1174754620451195"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findIdf(\"stories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times the word appears in text[i]\n",
    "\n",
    "def findTf(word, text):\n",
    "    words = text\n",
    "    c = 0\n",
    "    for w in words: \n",
    "        if w == word:\n",
    "            c += 1\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [puncFilter(c, True) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 0.09156860103399361, 3.221848749616356, 1.1174754620451195)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findTf(\"a\", texts[0]), findIdf(\"a\"), findIdf(\"magnus\"), log(10000/763, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(word, text):\n",
    "    return findTf(word, text) * findIdf(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.10924374808178"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(\"magnus\", texts[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_5 = ['stories', 'magician', 'psychic', 'writing', 'wonder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"stories \t idf: 1.117475 \t tf-idf:1.117475\"\n",
      "\"magician \t idf: 2.657577 \t tf-idf:2.657577\"\n",
      "\"psychic \t idf: 2.602060 \t tf-idf:5.204120\"\n",
      "\"writing \t idf: 0.997834 \t tf-idf:0.997834\"\n",
      "\"wonder \t idf: 1.767004 \t tf-idf:1.767004\"\n"
     ]
    }
   ],
   "source": [
    "for w in words_5:\n",
    "    print('\"%s \\t idf: %f \\t tf-idf:%f\"' % (w, findIdf(w), tfidf(w, texts[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Adapt your unigram model to use the tfidf scores of words, rather than a \n",
    "    bag-of-words representation. That is, rather than your features containing \n",
    "    the word counts for the 1000 most common unigrams, it should contain tfidf \n",
    "    scores for the 1000 most common unigrams. Report the MSE of this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_1k = [x[1] for x in counts_uni[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['was', 'book']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_1k[11:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_Id = dict(zip(uni_1k, range(len(uni_1k))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_idf(datum):\n",
    "    feat = [0]*len(uni_1k) # 长度为1000\n",
    "    \n",
    "    r = list(puncFilter(datum, True))\n",
    "    for w in r:\n",
    "        if w in uni_1k:\n",
    "            feat[uni_Id[w]] = tfidf(w, r)\n",
    "    feat.append(1) #offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature_idf(c) for c in corpus]\n",
    "y = [d['rating'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9660150616760588\n"
     ]
    }
   ],
   "source": [
    "print(getMSE(predictions, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Which other review has the highest cosine similarity compared to the first review \n",
    "    (provide the review id, or the text of the review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = []\n",
    "for i in range(1, len(data)):\n",
    "    d = data[i]\n",
    "    similarity = cosine_similarity(X[0:1], X[i:i+1])[0,0]\n",
    "    cos_sim.append((similarity, d['review_id']))\n",
    "cos_sim.sort()\n",
    "cos_sim.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity: 0.34862531225799814\n",
      "review id: r81495268\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarity:\" , cos_sim[0][0])\n",
    "print(\"review id:\" , cos_sim[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Implement a validation pipeline for this same data, by randomly shuffling the\n",
    "    data, using 10,000 reviews for training, another 10,000 for validation, and \n",
    "    another 10,000 for testing.1 Consider regularization parameters in the range\n",
    "    {0.01, 0.1, 1, 10, 100}, and report MSEs on the test set for the model that\n",
    "    performs best on the validation set. Using this pipeline, compare the following\n",
    "    alternatives in terms of their performance:\n",
    "    • Unigrams vs. bigrams\n",
    "    • Removing punctuation vs. preserving it. The model that preserves punctuation\n",
    "    should treat punctuation characters as separate words, e.g. “Amazing!” would \n",
    "    become [‘amazing’, ‘!’]\n",
    "    • tfidf scores vs. word counts\n",
    "    In total you should compare 2 × 2 × 2 = 8 models, and produce a table comparing \n",
    "    their performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_All = list(parseDataFromFile(\"train_Category.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(data_All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_All[:10000]\n",
    "validation = data_All[10000:20000]\n",
    "test = data_All[20000:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [d['review_text'] for d in train]\n",
    "train_y = [d['rating'] for d in train]\n",
    "validation_x = [d['review_text'] for d in validation]\n",
    "validation_y = [d['rating'] for d in validation]\n",
    "test_x = [d['review_text'] for d in test]\n",
    "test_y = [d['rating'] for d in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_cnt =  defaultdict(int)\n",
    "bi_cnt =defaultdict(int)\n",
    "uni_cnt_ = defaultdict(int)\n",
    "bi_cnt_ = defaultdict(int)\n",
    "\n",
    "for text in train_x:\n",
    "    r = list(puncFilter(text, True))\n",
    "    for w in r:\n",
    "        uni_cnt[w] += 1\n",
    "    \n",
    "    r = list(bigrams(text, True))\n",
    "    for w in r:\n",
    "        bi_cnt[w] += 1\n",
    "    \n",
    "    r = list(puncFilter(text, False))\n",
    "    for w in r:\n",
    "        uni_cnt_[w] += 1\n",
    "    \n",
    "    r = list(bigrams(text, False))\n",
    "    for w in r:\n",
    "        bi_cnt_[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_uni = [(uni_cnt[w], w) for w in uni_cnt]\n",
    "cnt_bi = [(bi_cnt[w], w) for w in bi_cnt]\n",
    "cnt_uni_ = [(uni_cnt_[w], w) for w in uni_cnt_]\n",
    "cnt_bi_ = [(bi_cnt_[w], w) for w in bi_cnt_]\n",
    "cnt_uni.sort()\n",
    "cnt_uni.reverse()\n",
    "cnt_bi.sort()\n",
    "cnt_bi.reverse()\n",
    "cnt_uni_.sort()\n",
    "cnt_uni_.reverse()\n",
    "cnt_bi_.sort()\n",
    "cnt_bi_.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni = [x[1] for x in cnt_uni[:1000]]\n",
    "bi = [x[1] for x in cnt_bi[:1000]]\n",
    "uni_ = [x[1] for x in cnt_uni_[:1000]]\n",
    "bi_ = [x[1] for x in cnt_bi_[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_uni = dict(zip(uni, range(len(uni))))\n",
    "id_bi = dict(zip(bi, range(len(bi))))\n",
    "id_uni_ = dict(zip(uni_, range(len(uni_))))\n",
    "id_bi_ = dict(zip(bi_, range(len(bi_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_count(datum, dataset, uniBi, uniBiId, remove):\n",
    "    feat = [0]*len(dataset) # 长度为1000\n",
    "    r = list(uniBi(datum, remove))\n",
    "    for w in r:\n",
    "        if w in dataset:\n",
    "            feat[uniBiId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_tfidf(datum, dataset, uniBi, uniBiId, remove):\n",
    "    feat = [0]*len(dataset) # 长度为1000\n",
    "    r = list(uniBi(datum, remove))\n",
    "    for w in r:\n",
    "        if w in dataset:\n",
    "            feat[uniBiId[w]] = tfidf(w, datum)\n",
    "    feat.append(1) #offset\n",
    "    return feat            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamdas = [0.01, 0.1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(X, y, lamdas):\n",
    "    min_mse = 100\n",
    "    best_lam = None\n",
    "    for lam in lamdas:\n",
    "        clf = linear_model.Ridge(lam, fit_intercept=False) # MSE + 1.0 l2\n",
    "        clf.fit(X, y)\n",
    "        mse = getMSE(clf.predict(X), y)\n",
    "            \n",
    "        if mse < min_mse:\n",
    "            min_mse = mse\n",
    "            best_lam = lam\n",
    "            \n",
    "    return best_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_count(dataset, uniBi, uniBiId, remove):\n",
    "    X = [feature_count(c, dataset, uniBi, uniBiId, remove) for c in train_x]\n",
    "    y = train_y\n",
    "    valid_X = [feature_count(c, dataset, uniBi, uniBiId, remove) for c in validation_x] \n",
    "    lam = valid(valid_X, validation_y, lamdas)\n",
    "    clf = linear_model.Ridge(lam, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(X, y)\n",
    "    test_X = [feature_count(c, dataset, uniBi, uniBiId, remove) for c in test_x]\n",
    "    predictions = clf.predict(test_X)\n",
    "    mse = getMSE(predictions, test_y)\n",
    "    return lam, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_tfidf(dataset, uniBi, uniBiId, remove):\n",
    "    X = [feature_tfidf(c, dataset, uniBi, uniBiId, remove) for c in train_x]\n",
    "    y = train_y\n",
    "    valid_X = [feature_tfidf(c, dataset, uniBi, uniBiId, remove) for c in validation_x] \n",
    "    lam = valid(valid_X, validation_y, lamdas)\n",
    "    clf = linear_model.Ridge(lam, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(X, y)\n",
    "    test_X = [feature_count(c, dataset, uniBi, uniBiId, remove) for c in test_x]\n",
    "    predictions = clf.predict(test_X)\n",
    "    mse = getMSE(predictions, test_y)\n",
    "    return lam, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 1.1909286683314602)\n"
     ]
    }
   ],
   "source": [
    "# Uni, removing punctuation, word count\n",
    "lamd2, mse2 = pipeline_count(uni, puncFilter, id_uni, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 1.3219871724186665)\n"
     ]
    }
   ],
   "source": [
    "# Uni, removing punctuation, tfidf\n",
    "lamd2, mse2 = pipeline_tfidf(uni, puncFilter, id_uni, True)\n",
    "lamd2, mse2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 1.2300564220457386)\n"
     ]
    }
   ],
   "source": [
    "# Bi, removing punctuation, word count\n",
    "lamd3, mse3 = pipeline_count(bi, bigrams, id_bi, True)\n",
    "lamd3, mse3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 1.3294717385220551)\n"
     ]
    }
   ],
   "source": [
    "# Bi, removing punctuation, tfidf\n",
    "lamd4, mse4 = pipeline_tfidf(bi, bigrams, id_bi, True)\n",
    "lamd4, mse4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 1.1898371657126983)\n"
     ]
    }
   ],
   "source": [
    "# Uni, preserving punctuation, word count\n",
    "lamd5, mse5 = pipeline_count(uni_, puncFilter, id_uni_, False)\n",
    "lamd5, mse5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 1.3104545969416221)\n"
     ]
    }
   ],
   "source": [
    "# Uni, preserving punctuation, tfidf\n",
    "lamd6, mse6 = pipeline_tfidf(uni_, puncFilter, id_uni_, False)\n",
    "lamd6, mse6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 1.2420305825444256)\n"
     ]
    }
   ],
   "source": [
    "# Bi, removing punctuation, word count\n",
    "lamd7, mse7 = pipeline_count(bi_, bigrams, id_bi_, False)\n",
    "lamd7, mse7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.01, 1.3294717385220551)\n"
     ]
    }
   ],
   "source": [
    "# Bi, removing punctuation, tfidf\n",
    "lamd8, mse8 = pipeline_tfidf(bi_, bigrams, id_bi_, False)\n",
    "lamd8, mse8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Uni, removing punctuation, count \t lamda: 0.01 \t MSE: 1.190929\"\n",
      "\"Uni, removing punctuation, tfidf \t lamda: 0.01 \t MSE: 1.321987\"\n",
      "\"Bi, removing punctuation, count \t lamda: 0.01 \t MSE: 1.230056\"\n",
      "\"Bi, removing punctuation, tfidf \t lamda: 0.01 \t MSE: 1.329472\"\n",
      "\"Uni, preserving punctuation, count \t lamda: 0.01 \t MSE: 1.189837\"\n",
      "\"Uni, preserving punctuation, tfidf \t lamda: 0.01 \t MSE: 1.310455\"\n",
      "\"Bi, removing punctuation, count \t lamda: 0.01 \t MSE: 1.242031\"\n",
      "\"Bi, removing punctuation, tfidf \t lamda: 0.01 \t MSE: 1.329472\"\n"
     ]
    }
   ],
   "source": [
    "title = ['Uni, removing punctuation, count', \n",
    "        'Uni, removing punctuation, tfidf',\n",
    "        'Bi, removing punctuation, count',\n",
    "        'Bi, removing punctuation, tfidf',\n",
    "        'Uni, preserving punctuation, count',\n",
    "        'Uni, preserving punctuation, tfidf',\n",
    "        'Bi, removing punctuation, count',\n",
    "        'Bi, removing punctuation, tfidf']\n",
    "lambd = [lamd1, lamd2, lamd3, lamd4, lamd5, lamd6, lamd7, lamd8]\n",
    "msee = [mse1, mse2, mse3, mse4, mse5, mse6, mse7, mse8]\n",
    "for i in range(0,8):\n",
    "    print('\"%s \\t lamda: %.2f \\t MSE: %f\"' % (title[i], lambd[i], msee[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Unigram, removing punctuation, count the frequency\" \n",
    "# has the lowest MSE: 1.190929 when lambda is 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
